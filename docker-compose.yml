# This example builds and runs the cuda version
services:
  marker:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BASE_IMAGE=pytorch/pytorch:2.1.1-cuda12.1-cudnn8-devel
    command: python convert_single.py /input/thinkpython.pdf /output/thinkpython.md --parallel_factor 2 --max_pages 10
    shm_size: '12gb' # set this to the size of VRAM if possible
    volumes:
      - ./input:/input
      - ./output:/output
      - xdg_cache:/root/.cache
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TORCH_DEVICE=cuda
      - INFERENCE_RAM=12
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  xdg_cache: